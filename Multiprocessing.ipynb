{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7CTNSq1mdlE2jtoTaDd0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sylv0303/Git/blob/main/Multiprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import pyodbc\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "from pandas import DataFrame"
      ],
      "metadata": {
        "id": "VIB51TDI4b2Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyodbc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6aG_rXf47N1",
        "outputId": "f9976bd8-009d-4bba-d202-87b11d1e2b45"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyodbc\n",
            "  Downloading pyodbc-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (334 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/334.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/334.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.7/334.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyodbc\n",
            "Successfully installed pyodbc-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization and Multiprocessing\n",
        "Creation of two functions using multiprocessing for code optimization. This technique enables more than 40 data extractions to be parallelized simultaneously, depending on the number of CPU cores available on the machine. This is particularly useful for Big Data management. By parallelizing queries, execution time has been considerably reduced. Two functions have been created: extract data and extract data chunk. On the one hand, the extract data chunk function extracts data from the database and takes as input an extraction date, a database connection, a list of columns to be extracted and a parameter to activate or deactivate the exposure filter. This function executes a SQL query to extract the corresponding data and returns the extracted data as a DataFrame.\n"
      ],
      "metadata": {
        "id": "xN_jokdm4cjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data_chunk(extract_date: str, cnxn: pyodbc.Connection, cols_to_extract: List, expo_filter: bool = True) -> DataFrame:\n",
        "    with cnxn:\n",
        "        if expo_filter:\n",
        "            query = f\"\"\"SELECT {','.join(cols_to_extract)}, SUM(expo)\n",
        "                        FROM table_name\n",
        "                        GROUP BY {','.join(cols_to_extract)};\"\"\"\n",
        "        else:\n",
        "            query = f\"\"\"SELECT {','.join(cols_to_extract)}, SUM(expo)\n",
        "                        FROM table_name\n",
        "                        WHERE (date_encours = '{extract_date}')\n",
        "                        GROUP BY {','.join(cols_to_extract)};\"\"\"\n",
        "\n",
        "        df = pd.read_sql(query, con=cnxn)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "3bIo-gy50wvo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other hand, the extract data function uses multiprocessing to extract data from several dates in parallel. It takes the same input arguments and creates a partially applied extract data chunk function, parallelizing up to 40 SQL queries simultaneously. The results are then concatenated into a single DataFrame."
      ],
      "metadata": {
        "id": "lNOq1Z1x3IEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data(date_list: List, cnxn: pyodbc.Connection, cols_to_extract: List, expo_filter: bool) -> DataFrame:\n",
        "    partial_extract = partial(extract_data_chunk, cnxn=cnxn, cols_to_extract=cols_to_extract, expo_filter=expo_filter)\n",
        "\n",
        "    with Pool(40) as p:\n",
        "        res = p.map(partial_extract, date_list)\n",
        "        df = pd.concat(res).reset_index(drop=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "dq8cpfps06v3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n"
      ],
      "metadata": {
        "id": "_Iv3ld6f3Rug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dra_D_C(date_debut: str, date_fin: str, expo_filter: bool):\n",
        "  date_format = \"%Y-%m-%d\"\n",
        "  date_start = datetime.strptime(date_debut, date_format)\n",
        "  date_end = datetime.strptime(date_fin, date_format)\n",
        "\n",
        "  date_list = []\n",
        "\n",
        "  while date_start <= date_end:\n",
        "    date_list.append(date_start.strftime(date_format))\n",
        "    date_start += relativedelta(months=1)\n",
        "    data = extract_data(\n",
        "    date_list=date_list, cnxn=GRIDS_CONN, cols_to_extract=[\"date_encours\", \"easy_number\", \"dra_type\", \"score\"], expo_filter=expo_filter)\n",
        "\n",
        "  data[\"date_m1\"] = data[\"date_encours\"] + DateOffset(months=-1)\n",
        "\n",
        "  filter_dra_d = data[data[\"dra_type\"] == \"D\"]\n",
        "  filter_dra_c = data[data[\"dra_type\"] == \"C\"]\n",
        "\n",
        "  result = filter_dra_d.merge(filter_dra_c, left_on=[\"easy_number\", \"date_encours\"], right_on=[\"easy_number\", \"date_m1\"], how=\"inner\", suffixes=(\"_debut\", \"_fin\"))\n",
        "\n",
        "  result[\"score_fin\"] = pd.to_numeric(result[\"score_fin\"])\n",
        "  result[\"score_debut\"] = pd.to_numeric(result[\"score_debut\"])\n",
        "  result[\"score_diff\"] = result[\"score_fin\"] - result[\"score_debut\"]\n",
        "  out = (\n",
        "  result.groupby([\"score_debut\", \"score_diff\"])\n",
        "  .agg({\"easy_number\": \"count\", \"expo\": \"sum\"})\n",
        "  ).reset_index()\n",
        "\n",
        "return out\n"
      ],
      "metadata": {
        "id": "8igP7w-r3Wbi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}